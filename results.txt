Python files
------------
1. collect_labels - collects the labels from the pretty print dataset and saves it as collected_label.json file
2. collect_policy_text - collects all the policy text from the sanitized policies and saved it as collect_policy_text.json file
3. collect_combined_data - combines the labels and policy text and saves it as collect_combined_data.json file
4. collect_clean_combined_data - filters the invalid labels and saves the valid label dataset as collect_clean_combined_data.json
5. collect_label_support - collects the label support from the clean_combined_data.json file to label_support.json
6. collect_top_labels - collects the top label from label_support.json to x_top_labels.json
7. collect_top_label_dataset - collects the top labels dataset including the policy text as x_top_labels_dataset.json
8. baseline_model_tfidf - baseline model that classifies the multi labels using tfidf as a vectorizer
9. baseline_model_word2vec - baseline model that classifies multi labels using word2vec as a vectorizer
10. multiple_baseline_models - testing multiple classification model such as linear SVC, Randomforest, NaiveBayes, ect.
11. nn_cnn - cnn experiment 1 with one conv1d layer (w2v embedding)
12. neural_network_glove - shallow neural network to classify the labels with glove emebeddings (pre-trained)


################### Results for top 40 labels ####################
-------------------------------------------------
Multiple Baseline Model-word2vec (Binary Relevance)
-------------------------------------------------
# Classifier: <class 'sklearn.ensemble._forest.RandomForestClassifier'>
Binary Relevance Precision:  0.509
Binary Relevance Recall:  0.244
Binary Relevance F1-score: 0.305
Binary Relevance Hamming Loss: 0.033

# Classifier: <class 'sklearn.svm._classes.LinearSVC'>
Binary Relevance Precision:  0.367
Binary Relevance Recall:  0.3
Binary Relevance F1-score: 0.243
Binary Relevance Hamming Loss: 0.068

# Classifier: <class 'sklearn.svm._classes.SVC'>
Binary Relevance Precision:  0.222
Binary Relevance Recall:  0.163
Binary Relevance F1-score: 0.181
Binary Relevance Hamming Loss: 0.035

# Classifier: <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Binary Relevance Precision:  0.536
Binary Relevance Recall:  0.336
Binary Relevance F1-score: 0.381
Binary Relevance Hamming Loss: 0.038

# Classifier: <class 'sklearn.linear_model._logistic.LogisticRegression'>
Binary Relevance Precision:  0.549
Binary Relevance Recall:  0.251
Binary Relevance F1-score: 0.311
Binary Relevance Hamming Loss: 0.034

Classifier: <class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Binary Relevance Precision:  0.315
Binary Relevance Recall:  0.269
Binary Relevance F1-score: 0.235
Binary Relevance Hamming Loss: 0.064

Classifier: <class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Binary Relevance Precision:  0.419
Binary Relevance Recall:  0.336
Binary Relevance F1-score: 0.347
Binary Relevance Hamming Loss: 0.042

-------------------------------------------------
Multiple Baseline Model-word2vec (Classifier Chain)
-------------------------------------------------
# Classifier: <class 'sklearn.ensemble._forest.RandomForestClassifier'>
ClassifierChain Precision:  0.519
ClassifierChain Recall:  0.272
ClassifierChain F1-score: 0.319
ClassifierChain Hamming Loss: 0.032

# Classifier: <class 'sklearn.svm._classes.LinearSVC'>
ClassifierChain Precision:  0.315
ClassifierChain Recall:  0.293
ClassifierChain F1-score: 0.237
ClassifierChain Hamming Loss: 0.051

# Classifier: <class 'sklearn.svm._classes.SVC'>
ClassifierChain Precision:  0.301
ClassifierChain Recall:  0.252
ClassifierChain F1-score: 0.232
ClassifierChain Hamming Loss: 0.038

# Classifier: <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
ClassifierChain Precision:  0.464
ClassifierChain Recall:  0.322
ClassifierChain F1-score: 0.364
ClassifierChain Hamming Loss: 0.037

# Classifier: <class 'sklearn.linear_model._logistic.LogisticRegression'>
ClassifierChain Precision:  0.382
ClassifierChain Recall:  0.261
ClassifierChain F1-score: 0.265
ClassifierChain Hamming Loss: 0.041

Classifier: <class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
ClassifierChain Precision:  0.237
ClassifierChain Recall:  0.206
ClassifierChain F1-score: 0.173
ClassifierChain Hamming Loss: 0.054

Classifier: <class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Binary Relevance Precision:  0.288
Binary Relevance Recall:  0.169
Binary Relevance F1-score: 0.265
Binary Relevance Hamming Loss: 0.017


-------------------------------------------------
Multiple Baseline Model-word2vec (Label Powerset)
-------------------------------------------------
# Classifier: <class 'sklearn.ensemble._forest.RandomForestClassifier'>
Label Powerset Precision:  0.463
Label Powerset Recall:  0.352
Label Powerset F1-score: 0.352
Label Powerset Hamming Loss: 0.039

# Classifier: <class 'sklearn.svm._classes.LinearSVC'>
Label Powerset Precision:  0.167
Label Powerset Recall:  0.151
Label Powerset F1-score: 0.107
Label Powerset Hamming Loss: 0.091

# Classifier: <class 'sklearn.svm._classes.SVC'>
Label Powerset Precision:  0.37
Label Powerset Recall:  0.333
Label Powerset F1-score: 0.324
Label Powerset Hamming Loss: 0.041

# Classifier: <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Label Powerset Precision:  0.306
Label Powerset Recall:  0.135
Label Powerset F1-score: 0.149
Label Powerset Hamming Loss: 0.054

# Classifier: <class 'sklearn.linear_model._logistic.LogisticRegression'>
Label Powerset Precision:  0.432
Label Powerset Recall:  0.383
Label Powerset F1-score: 0.391
Label Powerset Hamming Loss: 0.041

Classifier: <class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Label Powerset Precision:  0.305
Label Powerset Recall:  0.264
Label Powerset F1-score: 0.243
Label Powerset Hamming Loss: 0.055


-------------------------------------------------
Multiple Baseline Model-tfidf (Binary Relevance)
-------------------------------------------------
# Classifier: <class 'sklearn.ensemble._forest.RandomForestClassifier'>
Binary Relevance Precision:  0.201
Binary Relevance Recall:  0.044
Binary Relevance F1-score: 0.144
Binary Relevance Hamming Loss: 0.016

# Classifier: <class 'sklearn.svm._classes.LinearSVC'>
Binary Relevance Precision:  0.344
Binary Relevance Recall:  0.195
Binary Relevance F1-score: 0.328
Binary Relevance Hamming Loss: 0.017

# Classifier: <class 'sklearn.svm._classes.SVC'>
Binary Relevance Precision:  0.376
Binary Relevance Recall:  0.119
Binary Relevance F1-score: 0.25
Binary Relevance Hamming Loss: 0.015

# Classifier: <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Binary Relevance Precision:  0.429
Binary Relevance Recall:  0.193
Binary Relevance F1-score: 0.318
Binary Relevance Hamming Loss: 0.016

# Classifier: <class 'sklearn.linear_model._logistic.LogisticRegression'>
Binary Relevance Precision:  0.413
Binary Relevance Recall:  0.161
Binary Relevance F1-score: 0.309
Binary Relevance Hamming Loss: 0.014


-------------------------------------------------
Baseline Model tf-idf (RandomForest)
-------------------------------------------------
# Classifier: <class 'sklearn.ensemble._forest.RandomForestClassifier'>
Precision:  0.59
Recall:  0.157
F1-score: 0.305
Hamming Loss: 0.031


-------------------------------------------------
Baseline Model Word2vec (RandomForest)
-------------------------------------------------
# Classifier: <class 'sklearn.ensemble._forest.RandomForestClassifier'>
Precision: 0.524
Recall:  0.252
F1-score: 0.371
Hamming Loss: 0.032


-------------------------------------------------
cnn network
-------------------------------------------------

# # bs-16, lr-0.001, dp-0.6-0.6, conv-160,4
-------------------------
loss: 0.14787784218788147
precision: 0.5360000133514404
recall: 0.5775862336158752
c_f1: 0.5729897022247314
--------------------------

#bs-16, lr-0.01, dp-0.5-0.5, conv1-220,3, conv1-220,4
loss: 0.0927027016878128
precision: 0.5697329640388489
recall: 0.5714285969734192
c_f1: 0.5692447423934937

#bs-16, lr-0.001, dp-0.6-0.6, conv-183,3
loss: 0.29696816205978394
precision: 0.3586800694465637
recall: 0.6983240246772766
c_f1: 0.47736337780952454

#bs-16, lr-0.01, dp-0.5-0.5, conv1-220,5, conv1-220,5
precision: 0.6067073345184326
recall: 0.5751445293426514
get_f1: 0.5829235315322876


#bs-16, lr-0.01, dp-0.5-0.5, conv1-220,3, conv1-220,3
loss: 0.10056645423173904
precision: 0.5894736647605896
recall: 0.6363636255264282
c_f1: 0.6027687191963196

optuna: Trial 22 finished with value: 0.5630239248275757 and parameters: {'n_layers': 2, 'dropout': 0.057091867380322646,
'filters': 38, 'kernel_sizes': 4, 'strides': 1,
'filters0': 221, 'kernel_sizes0': 3, 'strides0': 2, 'dropout0': 0.3474126836787301,
'filters1': 239, 'kernel_sizes1': 3, 'strides1': 1, 'dropout1': 0.0001691711045728704,
'learning_rate': 0.0009895651135503852, 'size': 8}.

-------------------------------------------------
Shallow Model
-------------------------------------------------
best f1 score:
Trial 33 finished with value: 0.4393022060394287 and parameters: {'n_layers': 2, 'dropout': 0.18856839437923495, 'n_units_l0': 393, 'dropout0': 0.5016000407888777, 'n_units_l1': 331, 'dropout1': 0.5582626952033339, 'learning_rate': 0.0002583387933114043, 'size': 42}.


-------------------------------------------------
Bidirectional-LSTM or LSTM-CNN Model
-------------------------------------------------
##bidire 128-dp:0.7, 135-dp:0.7   bs:16 lr-0.001 dp-0.6
precision_3: 0.5907335877418518
recall_3: 0.4540059268474579
c_f1: 0.5109829306602478



#bidirectional 185-dp: 0.6 190-dp:0.6  bs-12 lr-0.0001
precision_12: 0.49497488141059875
recall_12: 0.5533707737922668
c_f1: 0.5215594172477722

#bidire 128-dp:0.7, 128-dp:0.7   bs:16 lr-0.001
precision: 0.6168582439422607
recall: 0.47774481773376465
c_f1: 0.5344550609588623

#bidire 32-dp:0.7, 128-dp:0.7   bs:16 lr-0.001
precision_3: 0.5379746556282043
recall_3: 0.5044510364532471
c_f1: 0.5078116655349731

##bidire 16-dp:0.7, 128-dp:0.7   bs:16 lr-0.001
precision_4: 0.5014084577560425
recall_4: 0.5281898975372314
c_f1: 0.5054097771644592

##bidire 16-dp:0.7, 64-dp:0.7   bs:16 lr-0.001
precision_5: 0.528124988079071
recall_5: 0.501483678817749
c_f1: 0.49487948417663574

##bidire 16-dp:0.7, 88-dp:0.7   bs:16 lr-0.001
precision_6: 0.555891215801239
recall_6: 0.5459940433502197
c_f1: 0.546823263168335


##bidire 16-dp:0.7, 82-dp:0.7   bs:16 lr-0.001
precision_8: 0.5
recall_8: 0.5934718251228333
c_f1: 0.5286514163017273

##bidire 16-dp:0.7, 78-dp:0.7   bs:16 lr-0.001 dp-0.6
precision_11: 0.5289255976676941
recall_11: 0.5697329640388489
c_f1: 0.5491048693656921

##bidire 16-dp:0.7, 78-dp:0.7   bs:16 lr-0.001 dp-0.6
precision_12: 0.5432098507881165
recall_12: 0.5222551822662354
c_f1: 0.5204870104789734

##bidire 16-dp:0.7, 68-dp:0.7   bs:16 lr-0.001 dp-0.6
precision_17: 0.5488505959510803
recall_17: 0.5667656064033508
c_f1: 0.5569500923156738

# bidire 13-dp:0.7, 53-dp:0.7   bs:16 lr-0.001 dp-0.6
precision_25: 0.5074626803398132
recall_25: 0.6053412556648254
c_f1: 0.5440539717674255

# bidire 16-dp:0.7, 46-dp:0.7   bs:16 lr-0.001 dp-0.6
precision_29: 0.47152620553970337
recall_29: 0.6142433285713196
c_f1: 0.5269097685813904




# -------------top 10 popular labels----------------
Classifier: <class 'sklearn.ensemble._forest.RandomForestClassifier'>
Cleaning tags
Cleaning tags, done
training time taken: 3.31 seconds
prediction time taken.... 0.0 seconds
Binary Relevance Precision:  0.974
Binary Relevance Recall:  0.521
Binary Relevance F1-score: 0.679
Binary Relevance Hamming Loss: 0.056

Classifier: <class 'sklearn.svm._classes.LinearSVC'>
Cleaning tags
Cleaning tags, done
training time taken: 0.07 seconds
prediction time taken.... 0.0 seconds
Binary Relevance Precision:  0.777
Binary Relevance Recall:  0.676
Binary Relevance F1-score: 0.723
Binary Relevance Hamming Loss: 0.057

Classifier: <class 'sklearn.svm._classes.SVC'>
Cleaning tags
Cleaning tags, done
training time taken: 0.47 seconds
prediction time taken.... 0.0 seconds
Binary Relevance Precision:  0.955
Binary Relevance Recall:  0.571
Binary Relevance F1-score: 0.715
Binary Relevance Hamming Loss: 0.053

Classifier: <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Cleaning tags
Cleaning tags, done
training time taken: 6.07 seconds
prediction time taken.... 1.0 seconds
Binary Relevance Precision:  0.888
Binary Relevance Recall:  0.682
Binary Relevance F1-score: 0.772
Binary Relevance Hamming Loss: 0.048

Classifier: <class 'sklearn.linear_model._logistic.LogisticRegression'>
Cleaning tags
Cleaning tags, done
training time taken: 0.19 seconds
prediction time taken.... 0.0 seconds
Binary Relevance Precision:  0.917
Binary Relevance Recall:  0.654
Binary Relevance F1-score: 0.763
Binary Relevance Hamming Loss: 0.049

Classifier: <class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Cleaning tags
Cleaning tags, done
training time taken: 0.01 seconds
prediction time taken.... 0.0 seconds
Binary Relevance Precision:  0.842
Binary Relevance Recall:  0.701
Binary Relevance F1-score: 0.765
Binary Relevance Hamming Loss: 0.049

Classifier: <class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Cleaning tags
Cleaning tags, done
training time taken: 0.01 seconds
prediction time taken.... 0.0 seconds
Binary Relevance Precision:  0.797
Binary Relevance Recall:  0.694
Binary Relevance F1-score: 0.742
Binary Relevance Hamming Loss: 0.056


--------------------------------------------------------
Results from classifiying the data practices as labels
--------------------------------------------------------

CNN Model
---------
# Conv1d-64
loss: 0.3834798038005829
precision: 0.7540322542190552
recall: 0.6732673048973083
get_f1: 0.7186695337295532

#drop-0.6 Conv1d-64 drop-0.6 conv1d-128 drop-0.6 conv1d-128
loss: 0.2663356065750122
precision: 0.7713696956634521
recall: 0.6909593939781189
get_f1: 0.7332054376602173


# Conv1d-128
loss: 0.38435038924217224
precision: 0.7459095120429993
recall: 0.7000903487205505
get_f1: 0.7261269688606262

#drop-0.6 Conv1d-128
loss: 0.2608160376548767
precision: 0.7868338823318481
recall: 0.6747311949729919
get_f1: 0.730289936065673

#drop-0.6 Conv1d-128 drop-0.6 conv1d-64
loss: 0.3067905604839325
precision: 0.7282809615135193
recall: 0.7229357957839966
get_f1: 0.7253777384757996


#drop-0.6 Conv1d-220 drop-0.6 conv1d-220
precision: 0.7863741517066956
recall: 0.6085790991783142
get_f1: 0.6850076913833618


#drop-0.6 Conv1d-220 drop-0.6 conv1d-220
precision_1: 0.7156521677970886
recall_1: 0.7434507608413696
c_f1: 0.7242441177368164

#drop-0.6 Conv1d-128, drop-0.6 conv1d-128, drop-0.6 conv1d-164
loss: 0.26514407992362976
precision_2: 0.7366071343421936
recall_2: 0.7452574372291565
c_f1: 0.7427158355712891

